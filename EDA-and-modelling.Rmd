---
    output:
      html_document:
              
        toc: true
        toc_float: false
        toc_depth: 3
        number_sections: true
        
        code_folding: hide
        code_download: true
        
        fig_width: 9 
        fig_height: 5
        fig_align: "center"
        
        highlight: pygments
        theme: cerulean
        
        keep_md: true
        
    title: "Human Learning meets Machine Learning"
    subtitle: "1,200+ hours of piano practice"
    author: "by Peter Hontaru"
---

# Introduction

## Problem statement {-}

### the why {-}

Learning a piano piece is a time-intensive process. Like with most other things, we tend to overestimate our own ability and then become frustrated that we cannot learn and play that Chopin piece like a concert pianist after only 30 minutes of practice. Fortunately, unlike what you might hear on Wall Street, previous performance **is** indicative of future success.

There's also a secondary goal here to hopefully provide a source of inspiration for other people that have always thought to themselves "**one day I'll learn a musical instrument**". Any other skill qualifies here, though. I aim to be doing this by, at the very least, allowing for visibility into my own journey. If this is what you want, why not give it a try?

### the what {-}

**Can we predict how long it would take to learn a piano piece based on a number of factors? If so, which factors influenced the total amount of hours required to learn the piece the most?**

### context {-}

I started playing the piano in 2018 as a complete beginner and I've been tracking my practice time for around 2 and a half years. I've now decided to put that data to good use and see what interesting patterns I might be able to find and hopefully develop a tool that others might be able to use in their journeys.

Here's an example of a recent performance - I mainly play classical music but cannot help but love Elton John's music.

<div align="center">
   <iframe width="560" height="315" src="https://www.youtube.com/embed/3fhhBZyFCzM" frameborder="0" data-external="1" allowfullscreen>
   </iframe>
</div>

## Data collection {-}

- imputed conservative estimations for the first 10 months of the first year (Jan '18 to Oct '18) and on Excel spreadsheet for Nov '18
- everything from Dec '18 onwards was tracked using Toggl, a time-tracking app/tool
- time spent in piano lessons was not tracked/included (usually 2-3 hours total per month)
- the **Extract, Transform, Load** script is available in the **global.R** file of this repo;
- for security reasons, I am not able to share the API script as the token also gives the option to change/remove the profile data; the raw data however, is stored in the **raw data** folder of this repo (not having the API call in simply just means that it won't be up to date for the current year)

**Disclaimer**: I am not affiliated with Toggl. I started using it a few years ago because it provided all the functionality I needed and loved its minimalistic design. The standard membership, which I use, is free of charge.

# Key insights

## Summary:

* identified various trends in my practice habits
* pieces could take me anywhere from ~4 hours to 40+ hours of practice, subject to difficulty (as assessed by the ABRSM grade)
* the **Random Forest** model was shown to be the most optimal model *(bootstrap resampling, 25x)*
    -   **Rsquared** - 0.57
    -   **MAE** - 6.0 hours
    -   **RMSE** - 7.6 hours
* looking at the variability of errors, there is a tendency to over-predict for pieces that took very little time to learn and under-predict for the more difficult ones. There could be two main reasons for this:
  -   artificially inflating the number of hours spent on a piece by returning to it a second time (due to a recital performance, wanting to improve the interpretation further or simply just liking it enough to play it again)
  -   learning easier pieces later on in my journey which means I will learn them faster than expected (based on my earlier data where a piece of a similar difficulty took longer)
-   the most important variables were shown to be the **length of the piece**, **standard of playing**(performance vs casual) and **experience**(lifetime total practice before first practice session on each piece)

```{r echo=FALSE}
knitr::opts_chunk$set(
    echo = FALSE, # show all code
    tidy = FALSE, # cleaner code printing
    size = "small", # smaller code
    
    fig.path = "figures/", #graphics location
    out.width = "100%",

    message = FALSE,
    warning = FALSE
    )
```

```{r include=FALSE}
source("api.R")
source("global.R")
```

# Exploratory Data Analysis (EDA)

## Piano practice timeline

```{r fig.height=6, fig.width=15}
raw_data%>%
  group_by(Month_format, Month_Start)%>%
  summarise(Total_Duration = sum(Duration)/60)%>%
  ungroup()%>%
  mutate(Total_Duration2 = as.integer(cumsum(Total_Duration)),
         # period = ifelse(Month_Start < "2018-11-01", "estimated", "tracked"),
         max = as.integer(max(Total_Duration2)),
         max2 = ifelse(max > Total_Duration2, "", "today"))%>%

  ggplot(aes(Month_format, Total_Duration2, group = 1))+
  geom_line(size = 2, color = "#69b3a2")+
  geom_point(size = 5, color = "#69b3a2")+
  geom_area(alpha = 0.3, fill = "#69b3a2")+
  # tracked vs estimated
  geom_segment(aes(x = "Nov\n '18", xend = "Nov\n '18", y = 282, yend = 0), lty = "dashed", alpha = 0.5)+
  geom_text(x="Aug\n '18", y = 202/2,  size = 4, label = "estimated")+
  geom_text(x="Feb\n '19", y = 202/2,  size = 4, label = "tracked")+
  # start
  # geom_point(x="Jan\n '18", y = 25, size = 9, shape = 21, fill = "dark red", col = "black")+
  # geom_text(x="Jan\n '18", y = 25+100, size = 5, nudge_x = 2, label = "Day 1")+
  # grade 3
  geom_point(x="Oct\n '18", y = 253, size = 9, shape = 21, fill = "dark red", col = "black")+
  geom_text(x="Oct\n '18", y = 253+200, size = 5, label = "Grade 3")+
  geom_text(x="Oct\n '18", y = 253+100,  size = 5, label = "253 hours")+
  # grade 5
  geom_point(x="Oct\n '19", y = 675, size = 9, shape = 21, fill = "dark red", col = "black")+
  geom_text(x="Oct\n '19", y = 675+200,  size = 5, label = "Grade 5")+
  geom_text(x="Oct\n '19", y = 675+100,  size = 5, label = "675 hours")+
  # grade 6
  geom_point(x="Oct\n '20", y = 1078, size = 9, shape = 21, fill = "dark red", col = "black")+
  geom_text(x="Oct\n '20", y = 1078+200,  size = 5, label = "Grade 6")+
  geom_text(x="Oct\n '20", y = 1078+100,  size = 5, label = "1078 hours")+
  # NOW
  # geom_point(aes(x="May\n '21", y = as.numeric(max)), size = 9, shape = 21, fill = "dark red", col = "black")+
  # geom_text(aes(label = max2), nudge_y = 95, nudge_x = -0.5, size = 5)+
  labs(x = NULL,
       y = "Total hours of practice")+
  theme_minimal()+
  theme(legend.position = "top")
```

## How long did I practice per piece?

Based on the level at the time and the difficulty of the piece, we can see that each piece took around 10-30 hours of practice.

```{r timeline}
#knitr::opts_chunk$set(fig.path = "figures/animation/")

#timeline_animation <- 
raw_data%>%
  filter(Date_Start > as.Date("2018/11/01"))%>%
  filter(Completed == "Yes")%>%
  group_by(Project, Date_Start)%>%
  summarise(Duration = sum(Duration)/60)%>%
  mutate(Cumulative_Piece = cumsum(Duration),
         Month_Year = as.factor(as.yearmon(Date_Start)),
         Month_format = str_replace(Month_Year, " 20", "\n '"))%>%
  ungroup()%>%
  mutate(Cumulative_Total = cumsum(Duration))%>%
  filter(Project %notin% c("Technique", "General", "Sightreading"))%>%
  left_join(model_data%>%select(Level, Project, ABRSM), by = "Project")%>%
  
ggplot(aes(Date_Start, Cumulative_Piece, fill = Level)) +
  geom_point(size = 10, shape = 21, col = "black", alpha = 0.5) +
  scale_size(range = c(.1, 16), guide = FALSE) +
  labs(title = 'Year: {frame_time}',
       x = NULL,
       y = "Total practice time per piece (hours)")+
  scale_color_tron()+
  scale_fill_tron()+
  theme_minimal() +
  theme(legend.position = "top")+
  transition_time(Date_Start) +
  ease_aes('linear')+
  exit_fade() +
  shadow_mark(alpha = 0.1, size = 5)

# save animation as gif for later use
#animate(timeline_animation, duration = 5, fps = 20, renderer = gifski_renderer())
#anim_save("timeline_animation.gif", animation = last_animation(), path = "www")
```

## How consistent was my practice?

Generally, I've done pretty well to maintain a high level of consistency with the exception of August/December. This is usually where I tend to be away from home on annual leave, and thus, not have access to a piano.

```{r fig.height=6, fig.width=15}
raw_data%>%
  filter(Source != "Estimated")%>%
  mutate(current_month = as.factor(ifelse(Month_Start == max(Month_Start), "Yes", "No")))%>%
  group_by(Month_Year, Month_Start, Month_format, current_month)%>%
  summarise(Days_Practice = n_distinct(Date_Start),
            Total_Duration = sum(Duration))%>%
  mutate(Days_Total = ifelse(current_month == "Yes", day(today()), days_in_month(Month_Start)), # account for the fact that the current month is incomplete
         Days_Not_Practiced = Days_Total - Days_Practice,
         Avg_Duration = as.integer(Total_Duration/Days_Total),
         Consistency = round(Days_Practice / Days_Total * 100,2),
         Consistency_Status = ifelse(Consistency>=75, "Over 75%", "Under 75%"),
         Consistency_Status = factor(Consistency_Status, levels = c("Under 75%", "Over 75%")),
         Month_format = reorder(Month_format, Month_Year))%>%
  ungroup()%>%
  mutate(Average = sum(Days_Practice)/sum(Days_Total)*100)%>%
  
  ggplot(aes(Month_format, Consistency, fill = Consistency_Status))+
  geom_hline(aes(yintercept = Average), lty = "dashed", alpha = 0.5)+
  geom_col(group = 1, col = "black")+
  geom_text(aes(label = Days_Not_Practiced), size = 5, nudge_y = 3)+
  geom_text(x = "Mar\n '21", aes(y = Average+3),  size = 4, label = "average")+
  labs(x = NULL,
       fill = "Consistency status (target 75%)",
       subtitle = "Numbers indicate days without any practice within each month")+
  scale_color_tron()+
  scale_fill_tron()+
  theme_minimal()+
  theme(legend.position = "top",
        axis.title.y = element_text(size =15),
        text = element_text(size=15))
```

## Was there a trend in my amount of daily average practice? {.tabset .tabset-fade .tabset-pills}

We can see that my practice time was correlated with the consistency, where the average session was much shorter in the months I was away from the piano. There's also a trend where my practice close to an exam session was significantly higher than any other time of the year. **Can you spot in which month I had my exam in 2019? What about the end of 2020?**

*the average practice length per month includes the days in which I did not practice*

### overall {-}

```{r fig.height=6, fig.width=15}
raw_data%>%
  filter(Source != "Estimated")%>%
  mutate(current_month = as.factor(ifelse(Month_Start == max(Month_Start), "Yes", "No")))%>%
  group_by(Month_Year, Month_Start, Month_format, current_month)%>%
  summarise(Days_Practice = n_distinct(Date_Start),
            Total_Duration = sum(Duration))%>%
  mutate(Days_Total = ifelse(current_month == "Yes", day(today()), days_in_month(Month_Start)), # account for the fact that the current month is incomplete
         Avg_Duration = as.integer(Total_Duration/Days_Total),
         Avg_Duration_Status = ifelse(Avg_Duration < 60, "Less than one hour", "One hour"),
         Month_format = reorder(Month_format, Month_Year))%>%
  ungroup()%>%
  mutate(Average = sum(Total_Duration)/sum(Days_Total))%>%
  
  ggplot(aes(Month_format, Avg_Duration, fill = Avg_Duration_Status))+
  geom_hline(yintercept = 60, lty = "dashed", alpha = 0.5)+
  geom_col(col = "black")+
  labs(x = NULL,
       y = "Average practice session length (min)",
       fill = "Status")+
  geom_text(aes(label = Avg_Duration), nudge_y = 5, size = 5)+
  geom_text(x = "Mar\n '21", aes(y = Average+3),  size = 4, label = "average")+
  scale_color_tron()+
  scale_fill_tron()+
  theme_minimal()+
  theme(legend.position = "top",
        axis.title.y = element_text(size =15),
        text = element_text(size=15),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
```

### Year on Year {-}

Similar trends as before are apparent where my average daily session is longer before the exams than during any other time in the year and a dip in the months where I usually take most of my annual leave. I really do need to start picking up the pace and get back to where I used to be.

```{r fig.height=6, fig.width=15}
raw_data%>%
  group_by(Month_Year, Month_Start, Month_format, Month_Name, Year)%>%
  summarise(Days_Practice = n_distinct(Date_Start),
            Total_Duration = sum(Duration))%>%
  mutate(Days_Total = days_in_month(Month_Start),
         Avg_Duration = as.integer(Total_Duration/Days_Total),
         Avg_Duration_Status = ifelse(Avg_Duration < 60, "Less than one hour", "One hour"),
         Month_format = reorder(Month_format, Month_Year),
         size = as.factor(ifelse(Year == 2018, 1, 1.5)),
         label = ifelse(month(Month_Start) == 1, as.character(Year), ""))%>%
  
  ggplot(aes(Month_Name, Avg_Duration, group = Year, size = size))+
  geom_line(aes(col = Year))+
  geom_label_repel(aes(label = label, col = Year))+
  labs(x = NULL,
       fill = "Status")+
  scale_color_tron()+
  scale_fill_tron()+
  theme_minimal()+
  theme(legend.position = "none")
```

## Did COVID significantly impact my practice time? {.tabset .tabset-fade .tabset-pills}

### graph {-}

Despite a similar median, we can see that the practice sessions were less likely to be over 80 min after COVID. We can test if this was a significant impact with a t-test.

```{r fig.height=5, fig.width=9}
covid_start <- as.Date("2020/03/23")

inference <- raw_data%>%
  filter(Source != "Estimated")%>%
  mutate(Covid_Status = as.factor(ifelse(Date_Start < covid_start, "Before COVID", "After COVID")),
         Covid_Status = reorder(Covid_Status, desc(Covid_Status)))%>%
  group_by(Covid_Status, Date_Start)%>%
  summarise(Duration = sum(Duration))%>%
  ungroup()
  
  ggplot(inference, aes(Covid_Status, Duration, fill = Covid_Status))+
  geom_boxplot(varwidth = TRUE, col = "black")+
  labs(x = NULL,
       y = "Average practice session (min)")+
  scale_color_tron()+
  scale_fill_tron()+
  theme_minimal()+
  theme(legend.position = "none")
```

### skewness assumption {-}

Given the extremely low p-value, the Shapiro-Wilk normality test implies that the distribution of the data is significantly different from a normal distribution and that we cannot assume the normality assumption. However, we're working with the entire population dataset for each class and thus, unlike the independence of data, this assumption is not crucial.
  
```{r}
  inference %>% 
  select(Covid_Status, Duration) %>% 
  group_by(group = as.character(Covid_Status)) %>%
  do(tidy(shapiro.test(.$Duration)))%>%
  kbl(caption = "Shapiro-Wilk normality test")%>%
  kable_paper("hover", full_width = F)

```

### equal variances assumption {-}

We can see that with a large p value, we should fail to reject the null hypothesis (Ho) and conclude that we do not have evidence to believe that population variances are not equal. We can assume that the equal variances assumption was met for our t test

```{r}
tidy(leveneTest(inference$Duration~inference$Covid_Status))%>%
  kbl(caption = "Levene's test")%>%
  kable_paper("hover", full_width = F)
```

### t-test {-}

My practice sessions post-COVID are significantly shorter than those before the pandemic. This might be surprising, given that we were in the lockdown most of the time. However, I've been spending my time doing a few other things such as improving my technical skillset with R (this analysis wouldn't have been possible otherwise) and learning italian.

```{r}
t_test <- inference%>%
  t_test(Duration ~ Covid_Status, var.equal = TRUE)%>%
  add_significance()%>%
  kbl()%>%
  kable_paper("hover", full_width = F)

t_test
```

## What type of music do I tend to play? {.tabset .tabset-fade .tabset-pills}

### by genre {-}

```{r}
graph_practice <- function(variable, nudge){
  raw_data%>%
  filter(Genre %notin% c("Other", "Not applicable"))%>%
  group_by({{variable}})%>%
  summarise(Duration = as.integer(sum(Duration)/60))%>%
  arrange(desc(Duration))%>%
  head(10)%>%

  ggplot(aes(reorder({{variable}}, Duration), Duration, fill = Duration))+
  geom_col(show.legend = FALSE, col = "black", width = 1)+
  geom_text(aes(label = Duration), show.legend = FALSE, nudge_y = nudge, size = 5)+
  scale_fill_gradient(low="yellow", high="red")+
  labs(x = NULL,
       y = "Total hours of practice")+
  coord_flip()+
  theme_minimal()+
  theme(axis.text.x = element_blank(),
        axis.ticks = element_blank())
}

graph_practice(Genre, 15)
```

### by composer {-}

```{r}
graph_practice(Composer, 5)
```

### by piece {-}

```{r}
graph_practice(Project, 3)
```

## Relation between difficulty and number of practice hours {.tabset .tabset-fade .tabset-pills}

### ABRSM grade {-}

Simplified, ABRSM grades are a group of 8 exams based on their difficulty (1 - beginner to 8 - advanced). There are also diploma grades but those are extremely advanced, equivalent to university level studies and out of the scope of this analysis. 

More information can be found on their official website at https://gb.abrsm.org/en/exam-support/your-guide-to-abrsm-exams/

```{r}
model_data%>%
  mutate(Duration = Duration)%>%
  
  ggplot(aes(ABRSM, Duration, fill = ABRSM))+
  geom_boxplot(varwidth = TRUE, outlier.colour = "red")+
  labs(x = "ABRSM Grade",
       y = "Total practice hours",
       subtitle = "The higher the difficulty, the more time it takes to learn a piece")+
  scale_color_tron()+
  scale_fill_tron()+
  theme_minimal()+
  theme(legend.position = "none")
```

### level {-}

A further aggregation of ABRSM grades; this is helpful given the very limited dataset within each grade and much easier on the eye. This is an oversimplification but they're classified as:
  * 1-4: Beginner
  * 5-6: Intermediate
  * 7-8: Advanced

```{r}
model_data%>%
  mutate(Duration = Duration)%>%
  
  ggplot(aes(Level, Duration, fill = Level))+
  geom_boxplot(varwidth = TRUE, outlier.colour = "red")+
  scale_color_tron()+
  scale_fill_tron()+
  labs(x = "Level",
       y = "Total practice hours",
       subtitle = "The higher the difficulty, the more time it takes to learn a piece")+
  theme_minimal()+
  theme(legend.position = "none")
```

## What about the piece length?

```{r}
model_data%>%
  
  ggplot(aes(Length, Duration, group = 1))+
  geom_jitter(aes(col = Level), width = 0.5, height = 0.5, size = 3)+
  geom_smooth(method = "lm", se=FALSE)+
  labs(x = "Piece length (mins)",
       y = "Hours needed to learn a piece",
       subtitle = "There appears to be a linear trend between piece length and total practice time")+
  scale_color_tron()+
  scale_fill_tron()+
  theme_minimal()+
  theme(legend.position = "top")
```

## Learning effect - do pieces of the same difficulty become easier to learn with time?

We can spot a trend where the time required to learn a piece of a similar difficulty (ABRSM Grade) decreases as my ability to play the piano increases (as judged by cumulative hours of practice). We should keep this in mind and include it as a variable into our prediction model.

```{r}
model_data%>%
  
  ggplot(aes(Cumulative_Duration, Duration, group = 1))+
  geom_point(aes(col = Level), size = 3)+
  geom_smooth(method = "lm", se=FALSE)+
  labs(x = "Cumulative hours practiced before the first practice of each piece",
       y = "Hours needed to learn a piece",
       subtitle = "Pieces of a similar difficulty become faster to learn")+
  scale_color_tron()+
  scale_fill_tron()+
  theme_minimal()+
  theme(legend.position = "top")
```

## Does "pausing" a piece impact the total time required to learn it?

How do we differentiate between pieces that we learn once and those that we come back to repeatedly? Examples could include wanting to improve the playing further, loving it so much we wanted to relearn it, preparing it for a new recital performance, etc.

As anyone that ever played the piano knows, re-learning a piece, particularly after you "drop" it for a few months/years, results in a much better performance/understanding of the piece. I definitely found that to be true in my experience, particularly with my exam pieces.The downside is that these pieces take longer to learn.

```{r width = 13}
model_data%>%
  mutate(Project_formatted = str_replace_all(Project,"[^[:graph:]]", " "),
         Project_label = as.factor(ifelse(Max_Break > 31, Project_formatted, "")))%>%
  
  ggplot(aes(as.integer(Max_Break), Duration, col = Max_Break <= 31))+
  geom_point(size = 3)+
  geom_text_repel(aes(label = Project_label), size = 3, show.legend = FALSE)+
  scale_x_log10()+
  scale_color_tron(labels = c(TRUE, FALSE))+
  guides(colour = guide_legend(reverse=TRUE))+
  labs(x = "Maximum days passed between two consecutive sessions on the same piece (log scale)",
       y = "Hours needed to learn a piece",
       col = "Break (over 1 month)",
       subtitle = "Taking a break before finishing a piece might lead to more hours required to learn it")+
  theme_minimal()+
  theme(legend.position = "top")
```

## Repertoire

```{r}
model_data%>%
  select(-Days_Practiced, -Date_End, -Max_Break)%>%
  mutate(Duration = round(Duration),
         Project = Link,
         Length = round(Length, 1))%>%
  select(-Link)%>%
  arrange(desc(Date_Start))%>%
  rename(Experience = Cumulative_Duration,
         Started = Date_Start)%>%
  relocate(Project, Duration, Genre, Length, Standard, Experience, Break, ABRSM, Level, Started)%>%
  
  kbl(escape = FALSE,caption = "Repertoire (red - response variable; green - predictor variables)")%>%
  kable_paper(c("hover", "striped"), full_width = F)%>%
  column_spec(c(1), bold = T, color = "black")%>%
  column_spec(c(2), bold = T, color = "red")%>%
  column_spec(c(3:8), bold = T, color = "green")%>%
  scroll_box(height = "450px")
```

# Modelling

Question: **Can we predict how long it would take to learn a piano piece based on a number of factors? If so, which factors influenced the total amount of hours required to learn the piece the most?**

## outliers

We can see that there are some outliers in our dataset:

* no. 16 and no. 28 are two Advanced (Grade 7 pieces). These are the only two pieces within this category and will be removed as they are both outliers; they will be introduced as I learn more Advanced repertoire (it is also likely that no. 16 is significantly harder than grade 7, but it is a custom arrangement and cannot be assigned a specific grade)
* no. 14 is an extremely short movement of a piece (a few seconds long) that unites the first and third movement of the same piece and it took very little time to learn
* no 4 is a piece that I previously learnt but did not track the time spent on a piece (as I wasn't tracking individual times back then). It then took significantly less time to re-learn it

```{r fig.height=7, fig.width=10}
mod <- glm(Duration ~ ABRSM + Genre + Length + Cumulative_Duration + Standard, data = model_data)
cooksd <- cooks.distance(mod)

autoplot(mod, which = 4:6, nrow = 3, col = 1,
         data = model_data, 
         size = 2,
         colour = "Level", 
         label.size = 4)+
  theme_minimal()+
  scale_colour_tron()

model_data <- model_data%>%
  filter(ABRSM != 7)%>%
  filter(Project != "Clementi - Sonatina no 3 - Mov 2")%>%
  filter(Project != "Bach - Minuet in G - 114")%>%
  droplevels()
```

## missing values

There are no missing values in the modelling dataset following the ETL process.

## feature engineering

* **categorical**:
  * **ABRSM grade**: 1 to 8
  * **Genre**: Baroque, Classical, Romantic, Modern
  * **Break**: learning it continuously or setting it aside for a while (1 month minimum)
  * **Standard** of practice: public performance or casual (relative to someone's level of playing)
* **numerical**:
  * **Experience**: total hours practiced before the first practice session on each piece
  * **Length** of the piece: minutes

## near zero variance

We can see that the *Break* feature has low variance (a high ratio of the most common answer "No" to the second most common "Yes"). We can exclude this from the model.

```{r}
nearZeroVar(model_data, freqCut = 2, uniqueCut = 20, names = TRUE)
```

## pre-processing

Let's use some basic standardisation offered by the caret package such as **centering** (subtract mean from values) and **scaling** (divide values by standard deviation).

```{r eval=FALSE, include=FALSE}
set.seed(123)

# set a backup variable
backup <- model_data

y <- model_data$Duration

# take out our response variable temporarily as we do not want this to be processed
model_data <- model_data %>%
  select(-Duration)

# center and scale our data (BoxCox if needed)
preProcess_range_model <- preProcess(model_data, method=c("center", "scale")) 

model_data <- predict(preProcess_range_model, newdata = model_data)
 
# append the Y variable back on with original values
model_data$Duration <- y
```

## resampling

Given the small size of the dataset, bootstrapping resampling method will be applied. This will give multiple estimates of out-of-sample error, rather than a single estimate.

```{r}
set.seed(123)

train.control <- trainControl(method = "boot",
                              number = 25,
                              search = "random")
```

## model selection

```{r results='hide'}
# set number of clusters 
clusters <- 4

# run them all in parallel
cl <- makeCluster(clusters, type = "SOCK")
 
# register cluster train in paralel
registerDoSNOW(cl)

# train models
model_1 <- train(Duration ~ ABRSM + Genre + Length + Cumulative_Duration + Standard,
               data = model_data,
               method = "ranger",
               tuneLength = 100,
               preProcess = c("center", "scale"),
               trControl = train.control)


model_2 <- train(Duration ~ ABRSM + Genre + Length + Cumulative_Duration + Standard,
                data = model_data,
                method = "lmStepAIC",
                tuneLength = 100,
                preProcess = c("center", "scale"),
                trControl = train.control)


model_3 <- train(Duration ~ ABRSM + Genre + Length + Cumulative_Duration + Standard,
                data = model_data,
                method = "lm",
                tuneLength = 100,
                preProcess = c("center", "scale"),
                trControl = train.control)

model_4 <- train(Duration ~ ABRSM + Genre + Length + Cumulative_Duration + Standard,
                data = model_data,
                method = "ridge",
                tuneLength = 100,
                preProcess = c("center", "scale"),
                trControl = train.control)

model_5 <- train(Duration ~ ABRSM + Genre + Length + Cumulative_Duration + Standard,
                data = model_data,
                method = "rf",
                tuneLength = 100,
                preProcess = c("center", "scale"),
                trControl = train.control)

model_6 <- train(Duration ~ ABRSM + Genre + Length + Cumulative_Duration + Standard,
                data = model_data,
                method = "gbm",
                tuneLength = 100,
                preProcess = c("center", "scale"),
                trControl = train.control)

model_7 <- train(Duration ~ ABRSM + Genre + Length + Cumulative_Duration + Standard,
                data = model_data,
                method = "pls",
                tuneLength = 100,
                preProcess = c("center", "scale"),
                trControl = train.control)

# shut the instances of R down
stopCluster(cl)

# learning curves to indicate overfitting and underfitting
# hyper parameters 
# https://topepo.github.io/caret/model-training-and-tuning.html#model-training-and-parameter-tuning
# https://topepo.github.io/caret/random-hyperparameter-search.html
```

We chose the Random Forest model as it was the best performing model. It is known as a model which is:

* not very sensitive to outliers
* good for non-linearity
* however, variable importance can be biased if categorical variables have few levels (toward high levels) or are correlated

The model selected had the mtry parameter (number of randomly selected variables used at each split) equal to 6.

```{r}
model_list <- list(ranger = model_1, lmStepAIC = model_2, lm = model_3, ridge = model_4, rf = model_5, gbm = model_6, pls = model_7)

model_comparison <- resamples(model_list)

summary(model_comparison)
```

## model evaluation

Based on our regression model, it does not look like we have significant multicollinearity between the full model variables so we can continue with our full model of 6 variables.

```{r}
tidy(vif(model_3$finalModel))%>%
  rename(VIF = x)%>%
  mutate(VIF = round(VIF, 1))%>%
  arrange(desc(VIF))%>%
  kbl(caption = "Variance Inflation Factor (VIF)")%>%
  kable_paper("hover", full_width = F)
```

### actuals vs predictions

```{r}
selected_model <- model_5

#Saving the model
saveRDS(selected_model, file = "model.rda")

#get predictions
predictions <- predict(selected_model, model_data)

#create dataset
model_data2 <- model_data
model_data2$Predicted <- predictions
model_data2$Actual <- model_data$Duration
model_data2$Residuals <- model_data2$Actual - model_data2$Predicted

# model_data2 <- model_data%>%
#   mutate(Actual = as.numeric(Duration),
#          Predicted = as.numeric(predictions),
#          Residuals = Actual - Predicted)%>%
#   select(Predicted, Actual, Residuals, Project, Level, Genre)

#visualise predicted vs actual

p <- ggplot(model_data2, aes(Predicted, Actual, label = Residuals))+
  geom_point(aes(text = Project, fill = Level), size = 3, shape = 21, col = "black", alpha = 0.75)+
  geom_smooth(method = "loess", col = "red", lwd = 1, se = FALSE, group = 1)+
  geom_abline(lty = "dashed", lwd = 0.5, col = "gray")+
  coord_cartesian(xlim = c(0,50), ylim = c(0,50))+
  labs(col = NULL)+
  scale_fill_tron()+
  theme_minimal() +
  theme(legend.position = "top")

p

#ggplotly(p+labs(fill = "")) %>% layout(legend = list(orientation = "h", x = 0.4, y = 1.2))
```

### residual distribution {.tabset .tabset-fade .tabset-pills}

#### histogram {-}

We can see that the residuals are mostly situated around 0.

```{r}
ggplot(model_data2, aes(Residuals, fill = ..count..))+
  geom_histogram(binwidth = 1, col = "black")+
  geom_vline(aes(xintercept=mean(Residuals)), lwd = 1, lty = 2) +
  labs(x="Residuals",
       y= "Total occurences")+
  scale_fill_gradient(low="yellow", high="red")+
  theme_minimal()+
  theme(legend.position = "none")
```

#### QQ plot / normal probability plot of residuals {-}

Similar to the previous histogram, we can spot some deviations from the normal distribution. Overall, we can state that the residuals follow a normal distribution.

```{r}
ggplot(model_data2, aes(sample=Residuals))+
  stat_qq(fill = "#DC0000B2", size = 3, shape = 21, col = "black", alpha = 0.75)+
  stat_qq_line(col = "black")+
  theme_minimal()+
  scale_color_tron()+
  scale_fill_tron()+
  labs(x = "Theoretical quantiles", 
       y = "Standardised residuals")+
  theme(legend.position = "top")
```

### independence of residuals (and hence observations)

There seems to be a slight trend where newer pieces have a smaller residuals. This could mean a lack of independence from the order of data collection (the model predictions are based on my current level of playing).

```{r}
model_data2%>%
  arrange(Date_Start)%>%

 ggplot(aes(x = 1:nrow(.), y = Residuals)) + 
  labs(x = "Index", 
       y = "Residuals")+
  geom_hline(yintercept = 0, alpha = 0.5, size = 3, color = "grey52", lty = 1)+
  geom_point(fill = "#DC0000B2", size = 3, shape = 21, col = "black")+
  geom_smooth(method = "lm", lwd = 0.5, col = "black", se = FALSE, alpha = 0.75)+
  theme_minimal()+
  scale_color_tron()+
  scale_fill_tron()
```

### actuals versus residuals

Looking at the variability of errors, there is still a tendency to over-predict for pieces that took very little time to learn and under-predict for the more difficult ones. There could be two main reasons for this:

  -   artificially inflating the number of hours spent on a piece by returning to it a second time (due to a recital performance, wanting to improve the interpretation further or simply just liking it enough to play it again)
  -   learning easier pieces later on in my journey which means I will learn them faster than expected (based on my earlier data where a piece of a similar difficulty took longer)

```{r}
p <- ggplot(model_data2, aes(Actual, Residuals, label = Predicted))+
  geom_hline(yintercept = 0, size = 3, color = "grey52")+
  geom_point(aes(text = Project, fill = Level), size = 3, shape = 21, col = "black", alpha = 0.75)+
  geom_smooth(method = "loess", col = "red", se = FALSE, alpha = 0.5)+
  labs(col = NULL)+
  scale_fill_tron()+
  theme_minimal() +
  theme(legend.position = "top")

p

#ggplotly(p + labs(fill = "")) %>% layout(legend = list(orientation = "h",x = 0.4, y = 1.2))
```

## Linear Regression (LR) or Random Forest (RF)?

We can see that the Random Forest performed significantly better than the simpler Linear Regression model. This isn't surprising since there might be non-linear trends within the data, and RFs are known to be more accurate (at the cost of interpretability and computing power).

```{r}
tidy(compare_models(model_3, model_5))%>%
  kbl(caption = "Random Forest vs Linear Regression")%>%
  kable_paper("hover", full_width = F)
```

## How many predictors did the most optimal model have?

```{r predictors}
ggplot(model_5)+
  geom_line(lwd = 1.25, colour = "black")+
  geom_point(size = 3, shape = 21, fill = "orange", col = "black")+
  geom_point(aes(x = 5, y = 7.281298), size = 10, shape = 21, fill = "orange", col = "black")+
  labs(subtitle = "The most optimal model was that with 5 predictors")+
  scale_color_tron()+
  scale_fill_tron()+
  theme_minimal()
```

## What were the most important variables?

The most important variables were shown to be the **length of the piece**, **standard of playing**(performance vs casual) and **experience**(lifetime total practice before first practice session on each piece)

```{r factors}
imp <- as.matrix(varImp(model_5)$importance)%>%
  as.data.frame()%>%
  rename(Importance = Overall)%>%
  mutate(Feature = as.factor(rownames(.)),
         Feature = reorder(Feature, Importance))

ggplot(imp, aes(Feature, Importance))+
  geom_segment(aes(Feature, y = 0, xend = Feature, yend = Importance), col = "black", size = 1.5) +
  geom_point(size = 10, col = "orange")+
  geom_text(aes(label = paste(round(Importance), "%", sep = "")), color = "black", size = 3, check_overlap = TRUE)+
  scale_color_tron()+
  scale_fill_tron()+
  theme_minimal()+
  coord_flip()+
  labs(subtitle = "Variable importance ranking")+
  theme(axis.text.x =  element_blank(), 
        axis.ticks = element_blank())

#plot(varImp(model_5))
```

# Limitations

-   very **limited data** which did not allow for a train/test split; however, a bootstrap resampling method is known to be a good substitute
-   biased to **one person's** learning ability (others might learn faster or slower)
-   on top of total hours of practice, **quality of practice** is a significant factor which is not captured in this dataset
-   very **difficult to assess when a piece is "finished"** as you can always further improve on your interpretation
-   not all pieces had official **ABRSM ratings** and a few had to be estimated; even for those that do have an official rating, the difficulty of a piece is highly subjective to each pianist and hard to quantify with one number
-   **memorisation** might be a confounding variable that was not accounted for and it could lead to inflating the numbers required on a specific piece

# What's next?

-   keep practicing, gather more data and refresh this analysis + adjust the model
-   add a recommender tab to the shiny dashboard to recommend pieces based on specific features

# Hardest things about this analysis:

* the Extract-Transform-Load process - cleaning the "dirty data" and finding creative ways to input the data on the front-end of the app in order to make it reporting-friendly on the back-end
  * especially true for metadata such as Genre, Type of practice, Composer and Piece name, tag pieces as "work in progress", etc
* automate ways to differentiate between pieces that I came back to vs pieces I only studied once (such whether the maximum difference between two consecutive practice sessions exceeded a threshold)
* work with very limited data

# Interactive application

-   you can find an interactive display of this presentation, as well as the model in production at the [following link](https://peterhontaru.shinyapps.io/Piano-Journal/)
-   <https://peterhontaru.shinyapps.io/Piano-Journal/>

![Screenshot](www/screenshot.png)

```{r eval=FALSE, include=FALSE}
# bayesian t test + cohen's d test for effect size
```