ggplot(aes(Length, Duration, group = 1))+
geom_point(aes(col = Level))+
geom_smooth(method = "lm", se=FALSE)+
labs(x = "Piece length (mins)",
y = "Hours needed to learn a piece",
subtitle = "There appears to be a linear trend between piece length and total practice time")+
scale_color_tron()+
scale_fill_tron()+
theme_ipsum_es()+
theme(legend.position = "top")
model_data%>%
ggplot(aes(Length, Duration, group = 1))+
geom_point(aes(col = Level), size = 3)+
geom_smooth(method = "lm", se=FALSE)+
labs(x = "Piece length (mins)",
y = "Hours needed to learn a piece",
subtitle = "There appears to be a linear trend between piece length and total practice time")+
scale_color_tron()+
scale_fill_tron()+
theme_ipsum_es()+
theme(legend.position = "top")
model_data%>%
ggplot(aes(Length, Duration, group = 1))+
geom_jitter(aes(col = Level), width = 0.5, height = 0.5, size = 3)+
geom_smooth(method = "lm", se=FALSE)+
labs(x = "Piece length (mins)",
y = "Hours needed to learn a piece",
subtitle = "There appears to be a linear trend between piece length and total practice time")+
scale_color_tron()+
scale_fill_tron()+
theme_ipsum_es()+
theme(legend.position = "top")
model_data%>%
ggplot(aes(Cumulative_Duration, Duration, col = Level))+
geom_point()+
geom_smooth(method = "lm", se=FALSE)+
#facet_wrap(.~ABRSM)+
labs(x = "Cumulative hours practiced before the first practice of each piece",
y = "Hours needed to learn a piece")+
scale_color_tron()+
scale_fill_tron()+
theme_ipsum_es()+
theme(legend.position = "top")
model_data%>%
ggplot(aes(Cumulative_Duration, Duration, group = 1))+
geom_point(aes(col = Level))+
geom_smooth(method = "lm", se=FALSE)+
#facet_wrap(.~ABRSM)+
labs(x = "Cumulative hours practiced before the first practice of each piece",
y = "Hours needed to learn a piece")+
scale_color_tron()+
scale_fill_tron()+
theme_ipsum_es()+
theme(legend.position = "top")
model_data%>%
ggplot(aes(Cumulative_Duration, Duration, group = 1))+
geom_point(aes(col = Level))+
geom_smooth(method = "loess", se=FALSE)+
#facet_wrap(.~ABRSM)+
labs(x = "Cumulative hours practiced before the first practice of each piece",
y = "Hours needed to learn a piece")+
scale_color_tron()+
scale_fill_tron()+
theme_ipsum_es()+
theme(legend.position = "top")
```{r}
model_data%>%
ggplot(aes(Cumulative_Duration, Duration, group = 1))+
geom_point(aes(col = Level))+
geom_smooth(method = "lm", se=FALSE)+
#facet_wrap(.~ABRSM)+
labs(x = "Cumulative hours practiced before the first practice of each piece",
y = "Hours needed to learn a piece")+
scale_color_tron()+
scale_fill_tron()+
theme_ipsum_es()+
theme(legend.position = "top")
model_data%>%
ggplot(aes(Cumulative_Duration, Duration, group = 1))+
geom_point(aes(col = Level))+
geom_smooth(method = "lm", se=FALSE)+
#facet_wrap(.~ABRSM)+
labs(x = "Cumulative hours practiced before the first practice of each piece",
y = "Hours needed to learn a piece",
title = "Pieces of a similar difficulty become faster to learn")+
scale_color_tron()+
scale_fill_tron()+
theme_ipsum_es()+
theme(legend.position = "top")
model_data%>%
ggplot(aes(Cumulative_Duration, Duration, group = 1))+
geom_point(aes(col = Level))+
geom_smooth(method = "lm", se=FALSE)+
#facet_wrap(.~ABRSM)+
labs(x = "Cumulative hours practiced before the first practice of each piece",
y = "Hours needed to learn a piece",
subtitle = "Pieces of a similar difficulty become faster to learn")+
scale_color_tron()+
scale_fill_tron()+
theme_ipsum_es()+
theme(legend.position = "top")
max_break <- raw_data%>%
filter(Genre %notin% c("Other", "Not applicable"))%>%
arrange(Project)%>%
group_by(Project)%>%
summarise(Max_Break = max(Date_End - lag(Date_End), na.rm = TRUE))
model_data%>%
inner_join(max_break, by = "Project")%>%
mutate(Project_formatted = str_replace_all(Project,"[^[:graph:]]", " "),
Project_label = as.factor(ifelse(Max_Break > 31, Project_formatted, "")))%>%
ggplot(aes(as.integer(Max_Break), Duration, col = Max_Break <= 31))+
geom_point()+
geom_text_repel(aes(label = Project_label), size = 3, show.legend = FALSE)+
scale_x_log10()+
scale_color_tron(labels = c(TRUE, FALSE))+
guides(colour = guide_legend(reverse=TRUE))+
labs(x = "Maximum days passed between two consecutive sessions on the same piece (log scale)",
y = "Hours needed to learn a piece",
col = "Break (over 1 month)",
subtitle = "Taking a break before finishing a piece might lead to more hours required to learn it")+
theme_ipsum_es()+
theme(legend.position = "top")
model_data%>%
inner_join(max_break, by = "Project")%>%
mutate(Project_formatted = str_replace_all(Project,"[^[:graph:]]", " "),
Project_label = as.factor(ifelse(Max_Break > 31, Project_formatted, "")))%>%
ggplot(aes(as.integer(Max_Break), Duration, col = Max_Break <= 31))+
geom_point(size = 3)+
geom_text_repel(aes(label = Project_label), size = 3, show.legend = FALSE)+
scale_x_log10()+
scale_color_tron(labels = c(TRUE, FALSE))+
guides(colour = guide_legend(reverse=TRUE))+
labs(x = "Maximum days passed between two consecutive sessions on the same piece (log scale)",
y = "Hours needed to learn a piece",
col = "Break (over 1 month)",
subtitle = "Taking a break before finishing a piece might lead to more hours required to learn it")+
theme_ipsum_es()+
theme(legend.position = "top")
model_data <- model_data%>%filter(ABRSM != 7)%>%droplevels()
train.control <- trainControl(method = "boot",
number = 10,
search = "random")
set.seed(123)
model_data%>%
ggplot(aes(Cumulative_Duration, Duration, group = 1))+
geom_point(aes(col = Level), size = 3)+
geom_smooth(method = "lm", se=FALSE)+
#facet_wrap(.~ABRSM)+
labs(x = "Cumulative hours practiced before the first practice of each piece",
y = "Hours needed to learn a piece",
subtitle = "Pieces of a similar difficulty become faster to learn")+
scale_color_tron()+
scale_fill_tron()+
theme_ipsum_es()+
theme(legend.position = "top")
# set out model_data table by project (data by each project)
model_data <- raw_data%>%
filter(Genre %notin% c("Other", "Not applicable"))%>%
filter(Completed == "Yes")%>%
droplevels()%>%
group_by(Project, Genre)%>%
summarise(Duration = sum(Duration)/60,
Date_Start = min(Date_Start),
Date_End = max(Date_End),
Days_Practiced = Date_End - Date_Start)%>%
inner_join(table_existing_info, by = "Project")%>%
mutate(ABRSM = as.factor(ABRSM),
Level = as.factor(ifelse(ABRSM %in% c(1,2,3,4), "Beginner",
ifelse(ABRSM %in% c(5,6), "Intermediate", "Advanced"))),
Standard = as.factor(Standard),
Length = Length/60,
Level = fct_relevel(Level, levels = c("Beginner", "Intermediate", "Advanced")),
ABRSM = fct_relevel(ABRSM, levels = c("1", "2", "3", "4", "5", "6", "7", "8")))%>%
inner_join(Practice_by_Date, by = "Date_Start")%>%
inner_join(max_break, by = "Project")
View(model_data)
# set out model_data table by project (data by each project)
model_data <- raw_data%>%
filter(Genre %notin% c("Other", "Not applicable"))%>%
filter(Completed == "Yes")%>%
droplevels()%>%
group_by(Project, Genre)%>%
summarise(Duration = sum(Duration)/60,
Date_Start = min(Date_Start),
Date_End = max(Date_End),
Days_Practiced = Date_End - Date_Start)%>%
inner_join(table_existing_info, by = "Project")%>%
mutate(ABRSM = as.factor(ABRSM),
Level = as.factor(ifelse(ABRSM %in% c(1,2,3,4), "Beginner",
ifelse(ABRSM %in% c(5,6), "Intermediate", "Advanced"))),
Standard = as.factor(Standard),
Length = Length/60,
Level = fct_relevel(Level, levels = c("Beginner", "Intermediate", "Advanced")),
ABRSM = fct_relevel(ABRSM, levels = c("1", "2", "3", "4", "5", "6", "7", "8")))%>%
inner_join(Practice_by_Date, by = "Date_Start")%>%
inner_join(max_break, by = "Project")%>%
mutate(ifelse(Break = as.factor(Max_Break > 31, "Yes", "No")))
# set out model_data table by project (data by each project)
model_data <- raw_data%>%
filter(Genre %notin% c("Other", "Not applicable"))%>%
filter(Completed == "Yes")%>%
droplevels()%>%
group_by(Project, Genre)%>%
summarise(Duration = sum(Duration)/60,
Date_Start = min(Date_Start),
Date_End = max(Date_End),
Days_Practiced = Date_End - Date_Start)%>%
inner_join(table_existing_info, by = "Project")%>%
mutate(ABRSM = as.factor(ABRSM),
Level = as.factor(ifelse(ABRSM %in% c(1,2,3,4), "Beginner",
ifelse(ABRSM %in% c(5,6), "Intermediate", "Advanced"))),
Standard = as.factor(Standard),
Length = Length/60,
Level = fct_relevel(Level, levels = c("Beginner", "Intermediate", "Advanced")),
ABRSM = fct_relevel(ABRSM, levels = c("1", "2", "3", "4", "5", "6", "7", "8")))%>%
inner_join(Practice_by_Date, by = "Date_Start")%>%
inner_join(max_break, by = "Project")%>%
mutate(Break = as.factor(ifelse(Max_Break > 31, "Yes", "No")))
train.control <- trainControl(method = "boot",
number = 10,
search = "random")
set.seed(123)
# set number of clusters
clusters <- 4
# run them all in parallel
cl <- makeCluster(clusters, type = "SOCK")
# register cluster train in paralel
registerDoSNOW(cl)
# train models
model <- train(Duration ~ ABRSM + Genre + Length + Cumulative_Duration + Break + Standard,
data = model_data,
method = "ranger",
preProcess = c("center", "scale", "BoxCox"),
tuneLength = 100,
trControl = train.control)
model2 <- train(Duration ~ ABRSM + Genre + Length + Cumulative_Duration + Break + Standard,
data = model_data,
method = "lmStepAIC",
preProcess = c("center", "scale", "BoxCox"),
tuneLength = 100,
trControl = train.control)
model3 <- train(Duration ~ ABRSM + Genre + Length + Cumulative_Duration + Break + Standard,
data = model_data,
method = "lm",
preProcess = c("center", "scale", "BoxCox"),
tuneLength = 100,
trControl = train.control)
model4 <- train(Duration ~ ABRSM + Genre + Length + Cumulative_Duration + Break + Standard,
data = model_data,
method = "ridge",
preProcess = c("center", "scale", "BoxCox"),
tuneLength = 100,
trControl = train.control)
model5 <- train(Duration ~ ABRSM + Genre + Length + Cumulative_Duration + Break + Standard,
data = model_data,
method = "rf",
preProcess = c("center", "scale", "BoxCox"),
tuneLength = 100,
trControl = train.control)
model6 <- train(Duration ~ ABRSM + Genre + Length + Cumulative_Duration + Break + Standard,
data = model_data,
method = "gbm",
preProcess = c("center", "scale", "BoxCox"),
tuneLength = 100,
trControl = train.control)
model7 <- train(Duration ~ ABRSM + Genre + Length + Cumulative_Duration + Break + Standard,
data = model_data,
method = "pls",
preProcess = c("center", "scale", "BoxCox"),
tuneLength = 100,
trControl = train.control)
# shut the instances of R down
stopCluster(cl)
# compare models
model_list <- list(one = model, two = model2, three = model3, four = model4, five = model5, six = model6, seven = model7)
model_comparison <- resamples(model_list)
summary(model_comparison)
# Estimate accuracy based on different groups? why does the model perform badly there
# keep LM model for explanation or even RF
# correlation
# learning curves to indicate overfitting and underfitting
# transform days_practiced into something more like 1-2-3 based on 120 days? why is chopin so high
# hyper parameters
# https://topepo.github.io/caret/model-training-and-tuning.html#model-training-and-parameter-tuning
# https://topepo.github.io/caret/random-hyperparameter-search.html
tidy(vif(model3$finalModel))%>%
rename(VIF = x)%>%
mutate(VIF = round(VIF, 1))%>%
arrange(desc(VIF))%>%
kbl(caption = "Variance Inflation Factor (VIF)")%>%
kable_paper("hover", full_width = F)
selected_model <- model5
#Saving the model
saveRDS(selected_model, file = "model.rda")
#get predictions
predictions <- predict(selected_model, model_data)
#create dataset
model_data2 <- model_data
model_data2$Predicted <- predictions
model_data2$Actual <- model_data$Duration
model_data2$Residuals <- model_data2$Actual - model_data2$Predicted
# model_data2 <- model_data%>%
#   mutate(Actual = as.numeric(Duration),
#          Predicted = as.numeric(predictions),
#          Residuals = Actual - Predicted)%>%
#   select(Predicted, Actual, Residuals, Project, Level, Genre)
#visualise predicted vs actual
ggplotly(
ggplot(model_data2, aes(Predicted, Actual, label = Residuals, col = Level))+
geom_point(aes(text = Project))+
geom_smooth(method = "loess", col = "red", lwd = 1, se = FALSE)+
geom_abline(lty = "dashed", lwd = 0.5, col = "gray")+
coord_cartesian(xlim = c(0,50), ylim = c(0,50))+
labs(col = NULL)+
scale_color_tron()+
theme_ipsum_es() +
theme(legend.position = "top")
) %>%
layout(legend = list(
orientation = "h",
x = 0.4, y = 1.2))
ggplot(model_data2, aes(Residuals, fill = ..count..))+
geom_histogram(binwidth = 1, col = "black")+
geom_vline(aes(xintercept=mean(Residuals)), lwd = 1, lty = 2) +
labs(x="Residuals",
y= "Total occurences")+
scale_fill_gradient(low="yellow", high="red")+
theme_ipsum_es()+
theme(legend.position = "none")
ggplotly(
ggplot(model_data2, aes(Actual, Residuals, col = Level, label = Predicted))+
geom_hline(yintercept = 0, size = 3, color = "grey52")+
geom_point(aes(text = Project), alpha = 0.5)+
geom_smooth(method = "loess", col = "red", se = FALSE)+
labs(col = NULL)+
scale_color_tron()+
theme_ipsum_es()
) %>%
layout(legend = list(orientation = "h",x = 0.4, y = 1.2))
View(model_data)
model_data <- model_data%>%filter(ABRSM != 7)%>%droplevels()
# set number of clusters
clusters <- 4
# run them all in parallel
cl <- makeCluster(clusters, type = "SOCK")
# register cluster train in paralel
registerDoSNOW(cl)
# train models
model <- train(Duration ~ ABRSM + Genre + Length + Cumulative_Duration + Break + Standard,
data = model_data,
method = "ranger",
preProcess = c("center", "scale", "BoxCox"),
tuneLength = 100,
trControl = train.control)
model2 <- train(Duration ~ ABRSM + Genre + Length + Cumulative_Duration + Break + Standard,
data = model_data,
method = "lmStepAIC",
preProcess = c("center", "scale", "BoxCox"),
tuneLength = 100,
trControl = train.control)
model3 <- train(Duration ~ ABRSM + Genre + Length + Cumulative_Duration + Break + Standard,
data = model_data,
method = "lm",
preProcess = c("center", "scale", "BoxCox"),
tuneLength = 100,
trControl = train.control)
model4 <- train(Duration ~ ABRSM + Genre + Length + Cumulative_Duration + Break + Standard,
data = model_data,
method = "ridge",
preProcess = c("center", "scale", "BoxCox"),
tuneLength = 100,
trControl = train.control)
model5 <- train(Duration ~ ABRSM + Genre + Length + Cumulative_Duration + Break + Standard,
data = model_data,
method = "rf",
preProcess = c("center", "scale", "BoxCox"),
tuneLength = 100,
trControl = train.control)
model6 <- train(Duration ~ ABRSM + Genre + Length + Cumulative_Duration + Break + Standard,
data = model_data,
method = "gbm",
preProcess = c("center", "scale", "BoxCox"),
tuneLength = 100,
trControl = train.control)
model7 <- train(Duration ~ ABRSM + Genre + Length + Cumulative_Duration + Break + Standard,
data = model_data,
method = "pls",
preProcess = c("center", "scale", "BoxCox"),
tuneLength = 100,
trControl = train.control)
# shut the instances of R down
stopCluster(cl)
# compare models
model_list <- list(one = model, two = model2, three = model3, four = model4, five = model5, six = model6, seven = model7)
model_comparison <- resamples(model_list)
summary(model_comparison)
# Estimate accuracy based on different groups? why does the model perform badly there
# keep LM model for explanation or even RF
# correlation
# learning curves to indicate overfitting and underfitting
# transform days_practiced into something more like 1-2-3 based on 120 days? why is chopin so high
# hyper parameters
# https://topepo.github.io/caret/model-training-and-tuning.html#model-training-and-parameter-tuning
# https://topepo.github.io/caret/random-hyperparameter-search.html
tidy(vif(model3$finalModel))%>%
rename(VIF = x)%>%
mutate(VIF = round(VIF, 1))%>%
arrange(desc(VIF))%>%
kbl(caption = "Variance Inflation Factor (VIF)")%>%
kable_paper("hover", full_width = F)
selected_model <- model7
#Saving the model
saveRDS(selected_model, file = "model.rda")
#get predictions
predictions <- predict(selected_model, model_data)
#create dataset
model_data2 <- model_data
model_data2$Predicted <- predictions
model_data2$Actual <- model_data$Duration
model_data2$Residuals <- model_data2$Actual - model_data2$Predicted
# model_data2 <- model_data%>%
#   mutate(Actual = as.numeric(Duration),
#          Predicted = as.numeric(predictions),
#          Residuals = Actual - Predicted)%>%
#   select(Predicted, Actual, Residuals, Project, Level, Genre)
#visualise predicted vs actual
ggplotly(
ggplot(model_data2, aes(Predicted, Actual, label = Residuals, col = Level))+
geom_point(aes(text = Project))+
geom_smooth(method = "loess", col = "red", lwd = 1, se = FALSE)+
geom_abline(lty = "dashed", lwd = 0.5, col = "gray")+
coord_cartesian(xlim = c(0,50), ylim = c(0,50))+
labs(col = NULL)+
scale_color_tron()+
theme_ipsum_es() +
theme(legend.position = "top")
) %>%
layout(legend = list(
orientation = "h",
x = 0.4, y = 1.2))
ggplot(model_data2, aes(Residuals, fill = ..count..))+
geom_histogram(binwidth = 1, col = "black")+
geom_vline(aes(xintercept=mean(Residuals)), lwd = 1, lty = 2) +
labs(x="Residuals",
y= "Total occurences")+
scale_fill_gradient(low="yellow", high="red")+
theme_ipsum_es()+
theme(legend.position = "none")
ggplotly(
ggplot(model_data2, aes(Actual, Residuals, col = Level, label = Predicted))+
geom_hline(yintercept = 0, size = 3, color = "grey52")+
geom_point(aes(text = Project), alpha = 0.5)+
geom_smooth(method = "loess", col = "red", se = FALSE)+
labs(col = NULL)+
scale_color_tron()+
theme_ipsum_es()
) %>%
layout(legend = list(orientation = "h",x = 0.4, y = 1.2))
tidy(compare_models(model7, model5))%>%
kbl(caption = "Model 1 vs model 2")%>%
kable_paper("hover", full_width = F)
plot(model5, main = "The most optimal model was that with 7 predictors", col = "orange", lwd = 1.5)
imp <- as.matrix(varImp(model5)$importance)%>%
as.data.frame()%>%
rename(Importance = Overall)%>%
mutate(Feature = as.factor(rownames(.)),
Feature = reorder(Feature, Importance))
ggplot(imp, aes(Feature, Importance))+
geom_segment(aes(Feature, y = 0, xend = Feature, yend = Importance), col = "black", size = 1.5) +
geom_point(size = 10, col = "orange")+
geom_text(aes(label = paste(round(Importance), "%", sep = "")), color = "black", size = 3, check_overlap = TRUE)+
scale_color_tron()+
scale_fill_tron()+
theme_ipsum_es()+
coord_flip()+
labs(title = "Variable importance ranking")+
theme(axis.text.x =  element_blank(),
axis.ticks = element_blank())
imp <- as.matrix(varImp(model5)$importance)%>%
as.data.frame()%>%
rename(Importance = Overall)%>%
mutate(Feature = as.factor(rownames(.)),
Feature = reorder(Feature, Importance))
ggplot(imp, aes(Feature, Importance))+
geom_segment(aes(Feature, y = 0, xend = Feature, yend = Importance), col = "black", size = 1.5) +
geom_point(size = 10, col = "orange")+
geom_text(aes(label = paste(round(Importance), "%", sep = "")), color = "black", size = 3, check_overlap = TRUE)+
scale_color_tron()+
scale_fill_tron()+
theme_ipsum_es()+
coord_flip()+
labs(title = "Variable importance ranking")+
theme(axis.text.x =  element_blank(),
axis.ticks = element_blank())
plot(varImp(model5))
imp <- as.matrix(varImp(model5)$importance)%>%
as.data.frame()%>%
rename(Importance = Overall)%>%
mutate(Feature = as.factor(rownames(.)),
Feature = reorder(Feature, Importance))
ggplot(imp, aes(Feature, Importance))+
geom_segment(aes(Feature, y = 0, xend = Feature, yend = Importance), col = "black", size = 1.5) +
geom_point(size = 10, col = "orange")+
geom_text(aes(label = paste(round(Importance), "%", sep = "")), color = "black", size = 3, check_overlap = TRUE)+
scale_color_tron()+
scale_fill_tron()+
theme_ipsum_es()+
coord_flip()+
labs(title = "Variable importance ranking")+
theme(axis.text.x =  element_blank(),
axis.ticks = element_blank())
#plot(varImp(model5))
